{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6671c29a",
   "metadata": {},
   "source": [
    "# Clean & build the datasets\n",
    "\n",
    "### run this script after the sql extract, it will create .csv files with the data, and the data for NESDE as a .pkl file\n",
    "\n",
    "#### it will produce the following files:\n",
    "1. Blood_Coagulation_SI_*_hr_dataset.csv -- dataset of normalized discretized features\n",
    "2. Blood_Coagulation_aPTT_*_hr_dataset.csv -- dataset of aPTT measures over irregular time\n",
    "3. Blood_Coagulation_UH_*_hr_dataset.csv -- dataset of UH interventions over irregular time\n",
    "4. Blood_Coagulation_norm_vals.pkl -- normalization coefficients for the features, can be useful to understand the actual values.\n",
    "5. Blood_Coagulation_train.pkl -- trainset for NESDE\n",
    "6. Blood_Coagulation_test.pkl -- testset for NESDE\n",
    "\n",
    " **note that this script takes a while to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cba6c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import pickle\n",
    "\n",
    "# path to inputevents table from mimicIV, used for further cleaning (need to be set):\n",
    "input_events_path = \"./inputevents.csv\"\n",
    "\n",
    "# time window (hours) to average the side-information\n",
    "step_size = 4\n",
    "\n",
    "# number of hours without UH that would \"clean\" the UH from the patients' body, we use if for splitting trajectories.\n",
    "hep_null_step_split = int(48/step_size)\n",
    "\n",
    "# minimal number of aPTT samples:\n",
    "min_ptt_smp = 2\n",
    "\n",
    "# max tajectory time:\n",
    "max_trj_time = 6000\n",
    "\n",
    "# minimal # of different values of UH\n",
    "min_hep_dlen = 2\n",
    "\n",
    "# test set split ratio:\n",
    "test_ratio = 0.3\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343c023f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some dicts to help working with the data:\n",
    "d_itemid2name = dict()\n",
    "d_itemid2name[224145] =\t'Heparin_Dose'\n",
    "d_itemid2name[225152] =\t'Heparin_Sodium'\n",
    "d_itemid2name[227466] =\t'PTT'\n",
    "d_itemid2name[220235] =\t'CO2'\n",
    "d_itemid2name[220045] =\t'HR'\n",
    "d_itemid2name[220562] =\t'PTT'\n",
    "d_itemid2name[227456] =\t'Albumin'\n",
    "d_itemid2name[220574] =\t'Albumin'\n",
    "d_itemid2name[220051] =\t'Dia_BP'\n",
    "d_itemid2name[220050] =\t'Sys_BP'\n",
    "d_itemid2name[225651] =\t'Direct_Bili'\n",
    "d_itemid2name[225690] =\t'Total_Bili'\n",
    "d_itemid2name[220615] =\t'Creatinine'\n",
    "d_itemid2name[223900] =\t'GCS_Verbal'\n",
    "d_itemid2name[220739] =\t'GCS_Eye'\n",
    "d_itemid2name[223901] =\t'GCS_Motor'\n",
    "d_itemid2name[220545] =\t'Ht_serum'\n",
    "d_itemid2name[220228] =\t'Hb'\n",
    "d_itemid2name[227467] =\t'INR'\n",
    "d_itemid2name[220561] =\t'INR'\n",
    "d_itemid2name[223830] =\t'PH'\n",
    "d_itemid2name[225678] =\t'Platelet_Count'\n",
    "d_itemid2name[227457] =\t'Platelet_Count'\n",
    "d_itemid2name[227465] =\t'PT'\n",
    "d_itemid2name[220560] =\t'PT'\n",
    "d_itemid2name[220210] =\t'RR'\n",
    "d_itemid2name[220227] =\t'SAO2'\n",
    "d_itemid2name[223762] =\t'Temp_C'\n",
    "d_itemid2name[223761] =\t'Temp_F'\n",
    "d_itemid2name[227429] =\t'Troponin'\n",
    "d_itemid2name[220546] =\t'WBC'\n",
    "d_itemid2name[227468] =\t'Fibrinogen'\n",
    "d_itemid2name[220541] =\t'Fibrinogen'\n",
    "d_itemid2name[220612] =\t'CRP'\n",
    "d_itemid2name[227444] =\t'CRP'\n",
    "d_itemid2name[226512] =\t'Adm_Weight_Kg'\n",
    "d_itemid2name[226531] =\t'Adm_Weight_lb'\n",
    "d_itemid2name[220640] = 'Potassium'\n",
    "d_itemid2name[227442] = 'Potassium'\n",
    "d_itemid2name[225625] = 'non_ionized_Ca'\n",
    "d_itemid2name[220645] = 'Sodium'\n",
    "d_itemid2name[224639] = 'Daily_Weight'\n",
    "d_itemid2name[225636] = 'D_DIMER'\n",
    "\n",
    "d_labitemid2name = dict()\n",
    "d_labitemid2name[50861] = 'ALT'\n",
    "d_labitemid2name[50878] = 'AST'\n",
    "d_labitemid2name[51300] = 'WBC'\n",
    "d_labitemid2name[51274] = 'PT'\n",
    "d_labitemid2name[51275] = 'PTT'\n",
    "d_labitemid2name[50889] = 'CRP'\n",
    "d_labitemid2name[50912] = 'Creatinine'\n",
    "d_labitemid2name[51006] = 'Urea_Nitrogen'\n",
    "d_labitemid2name[50971] = 'Potassium'\n",
    "d_labitemid2name[50893] = 'Total_Ca'\n",
    "d_labitemid2name[50983] = 'Sodium'\n",
    "d_labitemid2name[52618] = 'Sodium'\n",
    "d_labitemid2name[50883] = 'Direct_Bili'\n",
    "d_labitemid2name[50885] = 'Total_Bili'\n",
    "d_labitemid2name[51221] = 'Ht'\n",
    "d_labitemid2name[51222] = 'Hb'\n",
    "d_labitemid2name[51237] = 'INR'\n",
    "d_labitemid2name[51265] = 'Platelet_Count'\n",
    "d_labitemid2name[51003] = 'Troponin'\n",
    "d_labitemid2name[52111] = 'Fibrinogen'\n",
    "d_labitemid2name[51214] = 'Fibrinogen'\n",
    "d_labitemid2name[51196] = 'D_DIMER'\n",
    "d_labitemid2name[50915] = 'D_DIMER'\n",
    "\n",
    "range_dic = dict()\n",
    "range_dic['Weight_Kg'] = (30,300)\n",
    "range_dic['Daily_Weight'] = (30,300)\n",
    "range_dic['Adm_Weight_Kg'] = (30,300)\n",
    "range_dic['Adm_Weight_lb'] = (30/0.453592,300/0.453592)\n",
    "range_dic['Creatinine'] = (0.1,10)\n",
    "range_dic['Potassium'] = (1.5,10)\n",
    "range_dic['Total_Bili'] = (0,20)\n",
    "range_dic['PH'] = (6.5,8.5)\n",
    "range_dic['Sodium'] = (115,170)\n",
    "range_dic['Total_Ca'] = (4,15)\n",
    "range_dic['WBC'] = (0,50)\n",
    "range_dic['non_ionized_Ca'] = (5,20)\n",
    "range_dic['SAO2'] = (0,100)\n",
    "range_dic['Hb'] = (5,20)\n",
    "range_dic['Anti-Xa'] = (0,2)\n",
    "range_dic['INR'] = (0.8,8)\n",
    "range_dic['Platelet_Count'] = (0,2000)\n",
    "range_dic['HR'] = (30,200)\n",
    "range_dic['Direct_Bili'] = (0,20)\n",
    "range_dic['Ht'] = (10,60)\n",
    "range_dic['Ht_serum'] = (10,60)\n",
    "range_dic['Temp_C'] = (32,42)\n",
    "range_dic['Temp_F'] = (89.6,107.6)\n",
    "range_dic['PTT'] = (10,120)\n",
    "range_dic['RR'] = (5,60)\n",
    "range_dic['Dia_BP'] = (20,200)\n",
    "range_dic['Sys_BP'] = (40,280)\n",
    "range_dic['Albumin'] = (0,6)\n",
    "range_dic['AST'] = (10,10000)\n",
    "range_dic['ALT'] = (10,8000)\n",
    "range_dic['CRP'] = (0,100)\n",
    "range_dic['PT'] = (10,120)\n",
    "range_dic['Troponin'] = (0,60)\n",
    "range_dic['Urea_Nitrogen'] = (0,300)\n",
    "range_dic['D_DIMER'] = (0,6000)\n",
    "range_dic['CO2'] = (0,180)\n",
    "range_dic['Fibrinogen'] = (0,1000)\n",
    "\n",
    "\n",
    "d_rev_name2itemid = dict()\n",
    "for key in d_itemid2name.keys():\n",
    "    if d_itemid2name[key] in d_rev_name2itemid.keys():\n",
    "        d_rev_name2itemid[d_itemid2name[key]].append(key)\n",
    "    else:\n",
    "        d_rev_name2itemid[d_itemid2name[key]] = [key]\n",
    "\n",
    "\n",
    "d_rev_name2labitemid = dict()\n",
    "for key in d_labitemid2name.keys():\n",
    "    if d_labitemid2name[key] in d_rev_name2labitemid.keys():\n",
    "        d_rev_name2labitemid[d_labitemid2name[key]].append(key)\n",
    "    else:\n",
    "        d_rev_name2labitemid[d_labitemid2name[key]] = [key]\n",
    "\n",
    "for key in d_rev_name2itemid.keys():\n",
    "    if key not in d_rev_name2labitemid.keys():\n",
    "        d_rev_name2labitemid[key] = []\n",
    "\n",
    "for key in d_rev_name2labitemid.keys():\n",
    "    if key not in d_rev_name2itemid.keys():\n",
    "        d_rev_name2itemid[key] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45b2d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data and initiate df:\n",
    "df = pd.read_csv('./BC_chartevents.csv', encoding='unicode_escape').drop_duplicates()\n",
    "df_labs = pd.read_csv('./BC_labevents.csv', encoding='unicode_escape').drop_duplicates()\n",
    "df_diags = pd.read_csv('./BC_diagnoses.csv', encoding='unicode_escape').drop_duplicates()\n",
    "df_labs['value'] = df_labs['value'].str.extract('(\\d+(\\.\\d+)?)',expand=False).astype(float)[0]\n",
    "inds_to_replace = df_labs['valuenum'].isna() & df_labs['value'].notna()\n",
    "df_labs['valuenum'] = np.where(inds_to_replace,df_labs['value'],df_labs['valuenum'])\n",
    "df_diags = df_diags.loc[df_diags['category'].notna()]\n",
    "diag_columns = df_diags['category'].unique().tolist()\n",
    "columns = set(d_rev_name2itemid.keys())\n",
    "columns |= set(d_rev_name2labitemid.keys())\n",
    "columns = list(columns)\n",
    "new_df_template = pd.DataFrame(columns=['timestep','stay_id','gender','age','ethnicity','hospital_expire_flag','admission_type','Weight_Kg'] + diag_columns + columns)\n",
    "df_cont_heparin_template = pd.DataFrame(columns=['stay_id','traj_ind','starttime','endtime','rate'])\n",
    "df_cont_ptt_template = pd.DataFrame(columns=['stay_id','traj_ind','charttime','value'])\n",
    "\n",
    "\n",
    "step_interval = 3600*step_size\n",
    "curr_step = 0\n",
    "data_dics = dict()\n",
    "df_dic = dict()\n",
    "cont_df_dic = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ffd448",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute some dicts to be used later:\n",
    "for stayid in df['stay_id'].unique().tolist():\n",
    "    df_dic[stayid] = new_df_template.copy(deep=True)\n",
    "    cont_df_dic[stayid] = {'heparin':df_cont_heparin_template.copy(deep=True),'ptt':df_cont_ptt_template.copy(deep=True)}\n",
    "    data_dics[stayid] = dict()\n",
    "    data_dics[stayid]['stay_id'] = stayid\n",
    "    data_dics[stayid]['gender'] = df.loc[df['stay_id'] == stayid]['gender'].unique().tolist()[0]\n",
    "    data_dics[stayid]['age'] = df.loc[df['stay_id'] == stayid]['age'].unique().tolist()[0]\n",
    "    data_dics[stayid]['ethnicity'] = df.loc[df['stay_id'] == stayid]['ethnicity'].unique().tolist()[0]\n",
    "    data_dics[stayid]['hospital_expire_flag'] = df.loc[df['stay_id'] == stayid]['hospital_expire_flag'].unique().tolist()[0]\n",
    "    data_dics[stayid]['admission_type'] = df.loc[df['stay_id'] == stayid]['admission_type'].unique().tolist()[0]\n",
    "    curr_diag = df_diags.loc[df_diags['stay_id'] == stayid]['category'].unique().tolist()\n",
    "    for dcol in diag_columns:\n",
    "        if dcol in curr_diag:\n",
    "            data_dics[stayid][dcol] = 1\n",
    "        else:\n",
    "            data_dics[stayid][dcol] = 0\n",
    "\n",
    "df_ce = df.loc[df['charttime'].notna()]\n",
    "df_ie = df.loc[df['amount'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a54225b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the dfs:\n",
    "# it would take a little while...\n",
    "while True:\n",
    "    print(\"current time step: \",curr_step)\n",
    "    ind1 = df_ce['charttime'] >= (curr_step * step_interval)\n",
    "    ind2 = df_ie['endtime'] >= (curr_step * step_interval)\n",
    "    ind3 = df_labs['charttime'] >= (curr_step * step_interval)\n",
    "    if not any(ind1) and not any(ind2) and not any(ind3):\n",
    "        break\n",
    "    valid_stay = set(df_ce.loc[ind1]['stay_id'].unique().tolist())\n",
    "    valid_stay |= set(df_ie.loc[ind2]['stay_id'].unique().tolist())\n",
    "    valid_stay |= set(df_labs.loc[ind3]['stay_id'].unique().tolist())\n",
    "    valid_stay = list(valid_stay)\n",
    "    ind1 = ind1 & (df_ce['charttime'] < ((curr_step + 1) * step_interval))\n",
    "    ind1 = ind1 & (df_ce['charttime'].notna())\n",
    "    ind2 = ind2 & (df_ie['starttime'] < ((curr_step + 1) * step_interval))\n",
    "    ind2 = ind2 & df_ie['starttime'].notna()\n",
    "    ind2 = ind2 & df_ie['endtime'].notna()\n",
    "    ind3 = ind3 & (df_labs['charttime'] < ((curr_step + 1) * step_interval))\n",
    "    ind3 = ind3 & df_labs['charttime'].notna()\n",
    "    for stayid in tqdm.tqdm(valid_stay):\n",
    "        indstayid_ce = df_ce['stay_id'] == stayid\n",
    "        indstayid_ie = df_ie['stay_id'] == stayid\n",
    "        indstayid_labs = df_labs['stay_id'] == stayid\n",
    "        ind11 = ind1 & indstayid_ce\n",
    "        ind22 = ind2 & indstayid_ie\n",
    "        ind33 = ind3 & indstayid_labs\n",
    "        curr_df_ce = df_ce.loc[ind11]\n",
    "        curr_df_ie = df_ie.loc[ind22]\n",
    "        curr_df_labs = df_labs.loc[ind33]\n",
    "\n",
    "        tmp_d = data_dics[stayid].copy()\n",
    "        tmp_d['timestep'] = curr_step\n",
    "        tmp_d['Weight_Kg'] = np.nan\n",
    "        for key in columns: \n",
    "            tmp_key_curr_df_ce = curr_df_ce.loc[curr_df_ce['itemid'].isin(d_rev_name2itemid[key])]\n",
    "            tmp_key_curr_df_ie = curr_df_ie.loc[curr_df_ie['itemid'].isin(d_rev_name2itemid[key])]\n",
    "            tmp_key_curr_df_labs = curr_df_labs.loc[curr_df_labs['itemid'].isin(d_rev_name2labitemid[key])]\n",
    "            if key in range_dic.keys():\n",
    "                rel_inds1 = tmp_key_curr_df_ce['valuenum'] >= range_dic[key][0]\n",
    "                rel_inds1 = rel_inds1 & (tmp_key_curr_df_ce['valuenum'] <= range_dic[key][1])\n",
    "                rel_inds2 = tmp_key_curr_df_labs['valuenum'] >= range_dic[key][0]\n",
    "                rel_inds2 = rel_inds2 & (tmp_key_curr_df_labs['valuenum'] <= range_dic[key][1])\n",
    "                tmp_key_curr_df_ce = tmp_key_curr_df_ce.loc[rel_inds1]\n",
    "                tmp_key_curr_df_labs = tmp_key_curr_df_labs.loc[rel_inds2]\n",
    "\n",
    "            assert len(tmp_key_curr_df_ce) == 0 or len(tmp_key_curr_df_ie) == 0\n",
    "            if key == 'PTT':\n",
    "                ptt_dic = dict()\n",
    "            if len(tmp_key_curr_df_ce) > 0:\n",
    "                if len(tmp_key_curr_df_labs) > 0:\n",
    "                    if any(tmp_key_curr_df_ce['valuenum'].notnull()):\n",
    "                        tcharts_df = tmp_key_curr_df_ce.loc[tmp_key_curr_df_ce['valuenum'].notnull()][['charttime','valuenum']]\n",
    "                        charts_times = tcharts_df['charttime'].tolist()\n",
    "                        charts_vals = tcharts_df['valuenum'].tolist()\n",
    "                        if key == 'PTT':\n",
    "                            for jj in range(len(charts_vals)):\n",
    "                                ptt_dic[charts_times[jj]] = charts_vals[jj]\n",
    "                    if any(tmp_key_curr_df_labs['valuenum'].notnull()):\n",
    "                        tlabs_df = tmp_key_curr_df_labs.loc[tmp_key_curr_df_labs['valuenum'].notnull()][['charttime','valuenum']]\n",
    "                        labs_times = tlabs_df['charttime'].tolist()\n",
    "                        labs_vals = tlabs_df['valuenum'].tolist()\n",
    "                        assert len(labs_vals) == len(labs_times)\n",
    "                        for jj in range(len(labs_vals)):\n",
    "                            if labs_times[jj] not in charts_times:\n",
    "                                charts_vals.append(labs_vals[jj])\n",
    "                                if key == 'PTT':\n",
    "                                    ptt_dic[labs_times[jj]] = labs_vals[jj]\n",
    "\n",
    "                    if key == 'PTT':\n",
    "                        ptt_times = list(ptt_dic.keys()).copy()\n",
    "                        ptt_times.sort()\n",
    "                        for ttime in ptt_times:\n",
    "                            cont_df_dic[stayid]['ptt'] = cont_df_dic[stayid]['ptt'].append({'stay_id':stayid,'traj_ind':0,'charttime':ttime,'value':ptt_dic[ttime]}, ignore_index=True)\n",
    "                    tmp_d[key] = np.asarray(charts_vals).mean()\n",
    "                else:\n",
    "                    if any(tmp_key_curr_df_ce['valuenum'].notnull()):\n",
    "                        tcharts_df = tmp_key_curr_df_ce.loc[tmp_key_curr_df_ce['valuenum'].notnull()][['charttime','valuenum']]\n",
    "                        charts_vals = tcharts_df['valuenum'].tolist()\n",
    "                        charts_times = tcharts_df['charttime'].tolist()\n",
    "                        tmp_d[key] = np.asarray(charts_vals).mean()\n",
    "                        if key == 'PTT':\n",
    "                            for jj in range(len(charts_vals)):\n",
    "                                ptt_dic[charts_times[jj]] = charts_vals[jj]\n",
    "                            ptt_times = list(ptt_dic.keys()).copy()\n",
    "                            ptt_times.sort()\n",
    "                            for ttime in ptt_times:\n",
    "                                cont_df_dic[stayid]['ptt'] = cont_df_dic[stayid]['ptt'].append({'stay_id': stayid, 'traj_ind': 0, 'charttime': ttime, 'value': ptt_dic[ttime]}, ignore_index=True)\n",
    "\n",
    "                    else:\n",
    "                        tmp_d[key] = np.nan\n",
    "            elif len(tmp_key_curr_df_ie) > 0:\n",
    "                val = 0\n",
    "                rel_w_inds = tmp_key_curr_df_ie['patientweight'] >= range_dic['Weight_Kg'][0]\n",
    "                rel_w_inds = rel_w_inds & (tmp_key_curr_df_ie['patientweight'] <= range_dic['Weight_Kg'][1])\n",
    "                tmp_d['Weight_Kg'] = tmp_key_curr_df_ie.loc[rel_w_inds]['patientweight'].mean()\n",
    "                for ind in tmp_key_curr_df_ie.index.tolist():\n",
    "\n",
    "                    term1 = tmp_key_curr_df_ie.at[ind,'starttime'] >= (curr_step * step_interval)\n",
    "                    term2 = tmp_key_curr_df_ie.at[ind,'endtime'] < ((curr_step + 1) * step_interval)\n",
    "                    if key == 'Heparin_Sodium':\n",
    "                        if tmp_key_curr_df_ie.at[ind,'endtime'] == tmp_key_curr_df_ie.at[ind,'starttime']:\n",
    "                            rate = tmp_key_curr_df_ie.at[ind,'amount']\n",
    "                            cont_df_dic[stayid]['heparin'] = cont_df_dic[stayid]['heparin'].append({'stay_id': stayid, 'traj_ind': 0, 'starttime': tmp_key_curr_df_ie.at[ind, 'starttime'],'endtime': tmp_key_curr_df_ie.at[ind, 'endtime'] + 1, 'rate': rate}, ignore_index=True)\n",
    "                        else:\n",
    "                            rate = tmp_key_curr_df_ie.at[ind,'amount'] / (tmp_key_curr_df_ie.at[ind,'endtime'] - tmp_key_curr_df_ie.at[ind,'starttime'])\n",
    "                            cont_df_dic[stayid]['heparin'] = cont_df_dic[stayid]['heparin'].append({'stay_id':stayid,'traj_ind':0,'starttime':tmp_key_curr_df_ie.at[ind,'starttime'],'endtime':tmp_key_curr_df_ie.at[ind,'endtime'],'rate':rate}, ignore_index=True)\n",
    "                    if term1 and term2:\n",
    "                        val += tmp_key_curr_df_ie.at[ind,'amount']\n",
    "                    elif term1:\n",
    "                        val += (tmp_key_curr_df_ie.at[ind,'amount'] * ((curr_step + 1) * step_interval - tmp_key_curr_df_ie.at[ind,'starttime']))/(tmp_key_curr_df_ie.at[ind,'endtime'] - tmp_key_curr_df_ie.at[ind,'starttime'])\n",
    "                    elif term2:\n",
    "                        val += (tmp_key_curr_df_ie.at[ind,'amount'] * (tmp_key_curr_df_ie.at[ind,'endtime'] - curr_step * step_interval)) / (tmp_key_curr_df_ie.at[ind,'endtime'] - tmp_key_curr_df_ie.at[ind,'starttime'])\n",
    "                    else:\n",
    "                        val += (tmp_key_curr_df_ie.at[ind,'amount'] * step_interval)/(tmp_key_curr_df_ie.at[ind,'endtime'] - tmp_key_curr_df_ie.at[ind,'starttime'])\n",
    "                tmp_d[key] = val\n",
    "            elif len(tmp_key_curr_df_labs) > 0:\n",
    "                if any(tmp_key_curr_df_labs['valuenum'].notnull()):\n",
    "                    tlabs_df = tmp_key_curr_df_labs.loc[tmp_key_curr_df_labs['valuenum'].notnull()][['charttime', 'valuenum']]\n",
    "                    labs_times = tlabs_df['charttime'].tolist()\n",
    "                    labs_vals = tlabs_df['valuenum'].tolist()\n",
    "                    tmp_d[key] = np.asarray(labs_vals).mean()\n",
    "                    if key == 'PTT':\n",
    "                        for jj in range(len(labs_vals)):\n",
    "                            ptt_dic[labs_times[jj]] = labs_vals[jj]\n",
    "                        ptt_times = list(ptt_dic.keys()).copy()\n",
    "                        ptt_times.sort()\n",
    "                        for ttime in ptt_times:\n",
    "                            cont_df_dic[stayid]['ptt'] = cont_df_dic[stayid]['ptt'].append({'stay_id': stayid, 'traj_ind': 0, 'charttime': ttime, 'value': ptt_dic[ttime]}, ignore_index=True)\n",
    "                else:\n",
    "                    tmp_d[key] = np.nan\n",
    "            else:\n",
    "                tmp_d[key] = np.nan\n",
    "\n",
    "\n",
    "        df_dic[stayid] = df_dic[stayid].append(tmp_d,ignore_index=True)\n",
    "\n",
    "    curr_step += 1\n",
    "\n",
    "# backup, to save time in case things go wrong:\n",
    "with open('./tmp_BU.pkl','wb') as f:\n",
    "    pickle.dump({'df_dic':df_dic,'cont_df_dic':cont_df_dic},f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7397c1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load backup (uncomment and run):\n",
    "# with open('./tmp_BU.pkl','rb') as f:\n",
    "#     dd = pickle.load(f)\n",
    "# df_dic = dd['df_dic']\n",
    "# cont_df_dic = dd['cont_df_dic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf51bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# process the irregular data:\n",
    "new_df_dic = dict()\n",
    "key_list = list(df_dic.keys()).copy()\n",
    "patients_skipped = 0\n",
    "for stayid in tqdm.tqdm(key_list):\n",
    "    ts_ar = np.arange(len(df_dic[stayid]))\n",
    "    valid_hep_inds = df_dic[stayid].index[df_dic[stayid]['Heparin_Sodium'].notna()]\n",
    "    valid_arr = ts_ar[valid_hep_inds]\n",
    "    if (not any(df_dic[stayid]['Heparin_Sodium'].notna())) or len(valid_arr) < min_hep_dlen or any(df_dic[stayid]['Heparin_Dose'].notna()):\n",
    "#         print(\"Skipping patient: \", stayid)\n",
    "        patients_skipped += 1\n",
    "        del cont_df_dic[stayid]\n",
    "        continue\n",
    "\n",
    "    new_df_dic[stayid] = []\n",
    "    start_ind = 0\n",
    "    end_ind = ts_ar[-1]\n",
    "    if valid_arr[0] > hep_null_step_split:\n",
    "        start_ind = valid_arr[0] - hep_null_step_split\n",
    "    if valid_arr[-1] < ts_ar[-1] - hep_null_step_split:\n",
    "        end_ind = valid_arr[-1] + hep_null_step_split\n",
    "\n",
    "    split_inds = []\n",
    "    for i in range(1,len(valid_arr)):\n",
    "        if valid_arr[i - 1] + hep_null_step_split >= end_ind:\n",
    "            break\n",
    "        if valid_arr[i] > valid_arr[i-1] + hep_null_step_split:\n",
    "            split_inds.append(valid_arr[i-1] + np.ceil((valid_arr[i] - valid_arr[i-1])/2.0))\n",
    "    # crop beginning and end:\n",
    "    curr_start = start_ind - 1\n",
    "    curr_st = 0\n",
    "    if len(split_inds) > 0:\n",
    "        for ind in split_inds:\n",
    "            cropped_ind = ts_ar > curr_start\n",
    "            cropped_ind = cropped_ind & (ts_ar <= ind)\n",
    "            minidf = df_dic[stayid].loc[cropped_ind].copy(deep=True)\n",
    "            minidf = minidf.reset_index(drop=True)\n",
    "            new_df_dic[stayid].append(minidf.sort_values(by=['timestep']))\n",
    "            new_df_dic[stayid][-1]['timestep'] = np.arange(len(new_df_dic[stayid][-1]))\n",
    "            new_df_dic[stayid][-1]['traj_ind'] = curr_st*np.ones(len(new_df_dic[stayid][-1]))\n",
    "            ind_cont_hep = cont_df_dic[stayid]['heparin']['starttime'] >= ((curr_start + 1) * step_interval)\n",
    "            ind_cont_hep = ind_cont_hep & (cont_df_dic[stayid]['heparin']['starttime'] < ind * step_interval)\n",
    "            ind_cont_hep = cont_df_dic[stayid]['heparin'].index[ind_cont_hep]\n",
    "            ind_cont_ptt = cont_df_dic[stayid]['ptt']['charttime'] >= ((curr_start + 1) * step_interval)\n",
    "            ind_cont_ptt = ind_cont_ptt & (cont_df_dic[stayid]['ptt']['charttime'] < ind * step_interval)\n",
    "            ind_cont_ptt = cont_df_dic[stayid]['ptt'].index[ind_cont_ptt]\n",
    "            for iind in ind_cont_hep:\n",
    "                cont_df_dic[stayid]['heparin'].at[iind,'starttime'] = cont_df_dic[stayid]['heparin'].at[iind,'starttime'] - ((curr_start + 1) * step_interval)\n",
    "                cont_df_dic[stayid]['heparin'].at[iind,'endtime'] = cont_df_dic[stayid]['heparin'].at[iind,'endtime'] - ((curr_start + 1) * step_interval)\n",
    "                cont_df_dic[stayid]['heparin'].at[iind,'traj_ind'] = curr_st\n",
    "            for iind in ind_cont_ptt:\n",
    "                cont_df_dic[stayid]['ptt'].at[iind,'charttime'] = cont_df_dic[stayid]['ptt'].at[iind,'charttime'] - ((curr_start + 1) * step_interval)\n",
    "                cont_df_dic[stayid]['ptt'].at[iind,'traj_ind'] = curr_st\n",
    "\n",
    "            curr_start = ind\n",
    "            curr_st += 1\n",
    "    cropped_ind = ts_ar > curr_start\n",
    "    cropped_ind = cropped_ind & (ts_ar <= end_ind)\n",
    "    minidf = df_dic[stayid].loc[cropped_ind].copy(deep=True)\n",
    "    minidf = minidf.reset_index(drop=True)\n",
    "    new_df_dic[stayid].append(minidf.sort_values(by=['timestep']))\n",
    "    new_df_dic[stayid][-1]['timestep'] = np.arange(len(new_df_dic[stayid][-1]))\n",
    "    new_df_dic[stayid][-1]['traj_ind'] = curr_st*np.ones(len(new_df_dic[stayid][-1]))\n",
    "    del df_dic[stayid]\n",
    "    ind_cont_hep = cont_df_dic[stayid]['heparin']['starttime'] >= ((curr_start + 1) * step_interval)\n",
    "    ind_cont_hep = ind_cont_hep & (cont_df_dic[stayid]['heparin']['starttime'] < end_ind * step_interval)\n",
    "    ind_cont_hep = cont_df_dic[stayid]['heparin'].index[ind_cont_hep]\n",
    "    ind_cont_ptt = cont_df_dic[stayid]['ptt']['charttime'] >= ((curr_start + 1) * step_interval)\n",
    "    ind_cont_ptt = ind_cont_ptt & (cont_df_dic[stayid]['ptt']['charttime'] < end_ind * step_interval)\n",
    "    ind_cont_ptt = cont_df_dic[stayid]['ptt'].index[ind_cont_ptt]\n",
    "\n",
    "    for iind in ind_cont_hep:\n",
    "        cont_df_dic[stayid]['heparin'].at[iind, 'starttime'] = cont_df_dic[stayid]['heparin'].at[iind, 'starttime'] - ((curr_start + 1) * step_interval)\n",
    "        cont_df_dic[stayid]['heparin'].at[iind, 'endtime'] = cont_df_dic[stayid]['heparin'].at[iind, 'endtime'] - ((curr_start + 1) * step_interval)\n",
    "        cont_df_dic[stayid]['heparin'].at[iind, 'traj_ind'] = curr_st\n",
    "    for iind in ind_cont_ptt:\n",
    "        cont_df_dic[stayid]['ptt'].at[iind, 'charttime'] = cont_df_dic[stayid]['ptt'].at[iind, 'charttime'] - ((curr_start + 1) * step_interval)\n",
    "        cont_df_dic[stayid]['ptt'].at[iind, 'traj_ind'] = curr_st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6a9538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reorganize the columns:\n",
    "list_df = []\n",
    "for stayid in new_df_dic.keys():\n",
    "    list_df = list_df + new_df_dic[stayid]\n",
    "final_df = pd.concat(list_df,ignore_index=True)\n",
    "curr_ind = 0\n",
    "new_columns = []\n",
    "for col in final_df.columns:\n",
    "    if curr_ind == 1:\n",
    "        new_columns.append('traj_ind')\n",
    "        curr_ind += 1\n",
    "    if col != 'traj_ind':\n",
    "        new_columns.append(col)\n",
    "        curr_ind += 1\n",
    "final_df = final_df[new_columns]\n",
    "\n",
    "\n",
    "hep_list_df = []\n",
    "ptt_list_df = []\n",
    "for stayid in cont_df_dic.keys():\n",
    "    hep_list_df.append(cont_df_dic[stayid]['heparin'])\n",
    "    ptt_list_df.append(cont_df_dic[stayid]['ptt'])\n",
    "final_hep_df = pd.concat(hep_list_df,ignore_index=True)\n",
    "final_ptt_df = pd.concat(ptt_list_df,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2c0967",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge temp:\n",
    "final_df[\"Temp_F\"] = (5.0/9.0)*(final_df[\"Temp_F\"] - 32)\n",
    "final_df.Temp_C.fillna(final_df.Temp_F,inplace=True)\n",
    "final_df = final_df.drop(columns=\"Temp_F\")\n",
    "\n",
    "# merge Ht:\n",
    "final_df.Ht.fillna(final_df.Ht_serum,inplace=True)\n",
    "final_df = final_df.drop(columns=\"Ht_serum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a260d4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute weight:\n",
    "for patid in tqdm.tqdm(np.unique(final_df['stay_id'].to_numpy())):\n",
    "    pat_idx = final_df['stay_id'] == patid\n",
    "    for traj in np.unique(final_df[pat_idx]['traj_ind'].to_numpy()):\n",
    "        traj_idx = final_df['traj_ind'] == traj\n",
    "        traj_idx = traj_idx & pat_idx\n",
    "        w_kg = None\n",
    "        w_kg_adm = None\n",
    "        w_kg_day = None\n",
    "        w_lb_adm = None\n",
    "        for tstep in np.sort(final_df[traj_idx]['timestep'].to_numpy()):\n",
    "            idxx = final_df['timestep'] == tstep\n",
    "            idxx = idxx & traj_idx\n",
    "            if final_df[idxx]['Weight_Kg'].isna().to_numpy()[0]:\n",
    "                if w_kg is not None:\n",
    "                    final_df.loc[idxx,'Weight_Kg'] = w_kg\n",
    "                else:\n",
    "                    f_idx = final_df['Weight_Kg'].notna()\n",
    "                    f_idx = f_idx & traj_idx\n",
    "                    if any(f_idx):\n",
    "                        w_kg = final_df[f_idx].sort_values(by=['timestep'])['Weight_Kg'].to_numpy()[0]   \n",
    "                        final_df.loc[idxx,'Weight_Kg'] = w_kg\n",
    "            else:\n",
    "                w_kg = final_df[idxx]['Weight_Kg'].to_numpy()[0]\n",
    "            if final_df[idxx]['Daily_Weight'].isna().to_numpy()[0]:\n",
    "                if w_kg_day is not None:\n",
    "                    final_df.loc[idxx,'Daily_Weight'] = w_kg_day\n",
    "                else:\n",
    "                    f_idx = final_df['Daily_Weight'].notna()\n",
    "                    f_idx = f_idx & traj_idx\n",
    "                    if any(f_idx):\n",
    "                        w_kg_day = final_df[f_idx].sort_values(by=['timestep'])['Daily_Weight'].to_numpy()[0]\n",
    "                        final_df.loc[idxx,'Daily_Weight'] = w_kg_day\n",
    "            else:\n",
    "                w_kg_day = final_df[idxx]['Daily_Weight'].to_numpy()[0]\n",
    "            if final_df[idxx]['Adm_Weight_Kg'].isna().to_numpy()[0]:\n",
    "                if w_kg_adm is not None:\n",
    "                    final_df.loc[idxx,'Adm_Weight_Kg'] = w_kg_adm\n",
    "                else:\n",
    "                    f_idx = final_df['Adm_Weight_Kg'].notna()\n",
    "                    f_idx = f_idx & traj_idx\n",
    "                    if any(f_idx):\n",
    "                        w_kg_adm = final_df[f_idx].sort_values(by=['timestep'])['Adm_Weight_Kg'].to_numpy()[0]\n",
    "                        final_df.loc[idxx,'Adm_Weight_Kg'] = w_kg_adm\n",
    "            else:\n",
    "                w_kg_adm = final_df[idxx]['Adm_Weight_Kg'].to_numpy()[0]\n",
    "            if final_df[idxx]['Adm_Weight_lb'].isna().to_numpy()[0]:\n",
    "                if w_lb_adm is not None:\n",
    "                    final_df.loc[idxx,'Adm_Weight_lb'] = w_lb_adm\n",
    "                else:\n",
    "                    f_idx = final_df['Adm_Weight_lb'].notna()\n",
    "                    f_idx = f_idx & traj_idx\n",
    "                    if any(f_idx):\n",
    "                        w_lb_adm = final_df[f_idx].sort_values(by=['timestep'])['Adm_Weight_lb'].to_numpy()[0]\n",
    "                        final_df.loc[idxx,'Adm_Weight_lb'] = w_lb_adm\n",
    "            else:\n",
    "                w_lb_adm = final_df[idxx]['Adm_Weight_lb'].to_numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a0eef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter patients that recieved other anticoagulants:\n",
    "input_events = pd.read_csv(input_events_path)\n",
    "drug_itemids = [225906,225908,225148,229781,225147] + [225975,229597,230044]\n",
    "invalid_stays = input_events[input_events['itemid'].isin(drug_itemids)]['stay_id']\n",
    "invalid_stays = np.unique(invalid_stays.to_numpy())\n",
    "input_events = None\n",
    "final_hep_df = final_hep_df[~final_hep_df['stay_id'].isin(invalid_stays)]\n",
    "final_ptt_df = final_ptt_df[~final_ptt_df['stay_id'].isin(invalid_stays)]\n",
    "final_df = final_df[~final_df['stay_id'].isin(invalid_stays)]\n",
    "\n",
    "# make sure that there are no nan values for weight:\n",
    "w_not_nan = final_df['Weight_Kg'].notna()\n",
    "w_not_nan = w_not_nan | final_df['Daily_Weight'].notna()\n",
    "w_not_nan = w_not_nan | final_df['Adm_Weight_Kg'].notna()\n",
    "w_not_nan = w_not_nan | final_df['Adm_Weight_lb'].notna()\n",
    "final_df = final_df[w_not_nan]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae67bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# leave only one weight column and clean patients that have no record of weight:\n",
    "final_df = final_df[final_df['Weight_Kg'].notna()]\n",
    "\n",
    "# only 2 patients are problematic (others have fully observed Weight_Kg at this point)\n",
    "final_df = final_df[~final_df['stay_id'].isin([34303520,37530120])]\n",
    "final_df = final_df.drop(columns=\"Daily_Weight\")\n",
    "final_df = final_df.drop(columns=\"Adm_Weight_lb\")\n",
    "final_df = final_df.drop(columns=\"Adm_Weight_Kg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ab5c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge parallel dosing:\n",
    "final_hep_df = final_hep_df.drop_duplicates()\n",
    "merged_final_hep_df = pd.DataFrame(columns = final_hep_df.columns)\n",
    "for stayid in tqdm.tqdm(final_hep_df['stay_id'].unique()):\n",
    "    stayid_idx = final_hep_df['stay_id'] == stayid\n",
    "    for trajid in final_hep_df.loc[stayid_idx]['traj_ind'].unique():\n",
    "        traj_idx = stayid_idx & (final_hep_df['traj_ind'] == trajid)\n",
    "        mini_df = final_hep_df.loc[traj_idx].sort_values(by=['starttime','endtime']).reset_index(drop=True)\n",
    "        mini_times = np.sort(np.unique(mini_df[['starttime', 'endtime']].to_numpy().reshape(-1)),axis=None)\n",
    "        starttimes = mini_times[:-1]\n",
    "        endtimes = mini_times[1:]\n",
    "        hep_vals = np.zeros_like(starttimes)\n",
    "        for l in range(len(mini_df)):\n",
    "            line_s_time = mini_df['starttime'][l]\n",
    "            line_e_time = mini_df['endtime'][l]\n",
    "            line_hep_val = mini_df['rate'][l]\n",
    "            locs = starttimes < line_e_time\n",
    "            locs = locs & (endtimes > line_s_time)\n",
    "            hep_vals[locs]  = hep_vals[locs] + line_hep_val\n",
    "\n",
    "        starttimes = starttimes.reshape(-1,1)\n",
    "        endtimes = endtimes.reshape(-1,1)\n",
    "        hep_vals = hep_vals.reshape(-1,1)\n",
    "        merged_mini_df = pd.DataFrame(np.concatenate((stayid*np.ones_like(starttimes),trajid*np.ones_like(starttimes),\n",
    "                                                      starttimes, endtimes, hep_vals),axis=1),columns=final_hep_df.columns)\n",
    "        merged_final_hep_df = merged_final_hep_df.append(merged_mini_df)\n",
    "\n",
    "\n",
    "final_hep_df = merged_final_hep_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235d3e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorize values, Normalize SI & UH, change timesteps to minutes\n",
    "\n",
    "# irregular data:\n",
    "final_hep_df['starttime'] = (1.0 / 60.0) * final_hep_df['starttime']\n",
    "final_hep_df['endtime'] = (1.0 / 60.0) * final_hep_df['endtime']\n",
    "final_ptt_df['charttime'] = (1.0 / 60.0) * final_ptt_df['charttime']\n",
    "final_ptt_df.rename(columns={'charttime': 'time'}, inplace=True)\n",
    "\n",
    "# convert the rate to minutes:\n",
    "final_hep_df['rate'] = 60.0 * final_hep_df['rate']\n",
    "\n",
    "\n",
    "# discretized data:\n",
    "stay_ids = final_df['stay_id']\n",
    "traj_inds = final_df['traj_ind']\n",
    "final_df['timestep'] = step_size * 60 * final_df['timestep']\n",
    "time = final_df['timestep']\n",
    "\n",
    "final_df.ethnicity = pd.Categorical(final_df.ethnicity)\n",
    "final_df.gender = pd.Categorical(final_df.gender)\n",
    "final_df.admission_type = pd.Categorical(final_df.admission_type)\n",
    "final_df[\"ethnicity\"] = final_df.ethnicity.cat.codes\n",
    "final_df[\"gender\"] = final_df.gender.cat.codes\n",
    "final_df[\"admission_type\"] = final_df.admission_type.cat.codes\n",
    "final_df.rename(columns={'timestep': 'time'}, inplace=True)\n",
    "\n",
    "# relevant columns\n",
    "dcols = ['time','stay_id','traj_ind','gender','age','ethnicity','admission_type','Weight_Kg','Renal','Infectious','Pulmonary','CVS', \\\n",
    "                 'Hematological','Met','Smoking','GI','Endocrine','Psych','Obes','GCS_Motor','Creatinine','Dia_BP', \\\n",
    "                 'Total_Ca','PT','Ht','GCS_Eye','Total_Bili','Potassium','RR','Troponin','Sys_BP','Urea_Nitrogen', \\\n",
    "                 'ALT','CO2','AST','Sodium','PH','WBC','Platelet_Count','Temp_C','INR','HR','GCS_Verbal','Hb', \\\n",
    "                 'non_ionized_Ca']\n",
    "final_df = final_df[dcols]\n",
    "\n",
    "# divide the UH rate by the patient's weight:\n",
    "final_hep_df['org_rate'] = final_hep_df['rate']\n",
    "for stayid in tqdm.tqdm(final_df['stay_id'].unique()):\n",
    "    stayid_idx = final_df['stay_id'] == stayid\n",
    "    stayid_idx_hep = final_hep_df['stay_id'] == stayid\n",
    "    for trajid in final_df.loc[stayid_idx]['traj_ind'].unique():\n",
    "        traj_idx = stayid_idx & (final_df['traj_ind'] == trajid)\n",
    "        traj_idx_hep = stayid_idx_hep & (final_hep_df['traj_ind'] == trajid)\n",
    "        indices = final_df.index[traj_idx].to_list()\n",
    "        for id,time in enumerate(final_df.loc[traj_idx]['time']):\n",
    "            weight = final_df.loc[traj_idx]['Weight_Kg'][indices[id]]\n",
    "            rel_heps = traj_idx_hep & (final_hep_df['starttime'] >= time)\n",
    "            rel_heps = rel_heps & (final_hep_df['starttime'] < time + 360)\n",
    "            final_hep_df.loc[rel_heps,'rate'] = (1.0 / weight) * final_hep_df.loc[rel_heps]['rate']\n",
    "\n",
    "# normalize SI:\n",
    "min_vals = final_df.min()\n",
    "max_vals = final_df.max()\n",
    "norm_vals_dic = {'min':min_vals.to_numpy()[3:],'max':max_vals.to_numpy()[3:]}\n",
    "\n",
    "# save normalization values, for inverse transformation;\n",
    "with open('./Blood_Coagulation_norm_vals.pkl','wb') as f:\n",
    "    pickle.dump(norm_vals_dic,f)\n",
    "final_df = (final_df - final_df.min()) / (final_df.max() - final_df.min())\n",
    "final_df['stay_id'] = stay_ids\n",
    "final_df['traj_ind'] = traj_inds\n",
    "final_df['time'] = time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c3d659",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop patients according to aPTT times:\n",
    "ptt_drop_idx = final_ptt_df['stay_id'].isna() # all false, no nans\n",
    "hep_drop_idx = final_hep_df['stay_id'].isna()\n",
    "drop_idx = final_df['stay_id'].isna()\n",
    "\n",
    "for stayid in final_ptt_df['stay_id'].unique():\n",
    "    ptt_stayid_idx = final_ptt_df['stay_id'] == stayid\n",
    "    hep_stayid_idx = final_hep_df['stay_id'] == stayid\n",
    "    stayid_idx = final_df['stay_id'] == stayid\n",
    "    for trajid in final_ptt_df.loc[ptt_stayid_idx]['traj_ind'].unique():\n",
    "        ptt_traj_idx = ptt_stayid_idx & (final_ptt_df['traj_ind'] == trajid)\n",
    "        hep_traj_idx = hep_stayid_idx & (final_hep_df['traj_ind'] == trajid)\n",
    "        traj_idx = stayid_idx & (final_df['traj_ind'] == trajid)\n",
    "        if (final_ptt_df.loc[ptt_traj_idx]['time'].max() > max_trj_time) or (len(final_ptt_df.loc[ptt_traj_idx]['time']) < min_ptt_smp):\n",
    "            ptt_drop_idx = (ptt_drop_idx | ptt_traj_idx)\n",
    "            hep_drop_idx = (hep_drop_idx | hep_traj_idx)\n",
    "            drop_idx = (drop_idx | traj_idx)\n",
    "            \n",
    "final_df.drop(final_df[drop_idx].index, inplace=True)\n",
    "final_ptt_df.drop(final_ptt_df[ptt_drop_idx].index, inplace=True)\n",
    "final_hep_df.drop(final_hep_df[hep_drop_idx].index, inplace=True)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e165a4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save final csvs:\n",
    "final_df.to_csv('./Blood_Coagulation_SI_' + str(step_size) + 'hr_dataset.csv',index=False)\n",
    "final_ptt_df.to_csv('./Blood_Coagulation_aPTT_' + str(step_size) + 'hr_dataset.csv',index=False)\n",
    "final_hep_df.to_csv('./Blood_Coagulation_UH_' + str(step_size) + 'hr_dataset.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1079cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bulid pickle file for the Dataloader:\n",
    "import torch\n",
    "import random\n",
    "\n",
    "out_path = './Blood_Coagulation_train.pkl'\n",
    "test_out_path = './Blood_Coagulation_test.pkl'\n",
    "si_path = './Blood_Coagulation_SI_' + str(step_size) + 'hr_dataset.csv'\n",
    "uh_path = './Blood_Coagulation_UH_' + str(step_size) + 'hr_dataset.csv'\n",
    "aptt_path = './Blood_Coagulation_aPTT_' + str(step_size) + 'hr_dataset.csv'\n",
    "\n",
    "SI = pd.read_csv(si_path, encoding='unicode_escape')\n",
    "UH = pd.read_csv(uh_path, encoding='unicode_escape')\n",
    "aPTT = pd.read_csv(aptt_path, encoding='unicode_escape')\n",
    "\n",
    "data = []\n",
    "# only running on aPTT indices because without aPTT impossible to train/eval\n",
    "for stayid in tqdm(aPTT['stay_id'].unique()):\n",
    "    SI_stay_idx = SI['stay_id'] == stayid\n",
    "    UH_stay_idx = UH['stay_id'] == stayid\n",
    "    aPTT_stay_idx = aPTT['stay_id'] == stayid\n",
    "    for trajid in aPTT[aPTT_stay_idx]['traj_ind'].unique():\n",
    "        SI_traj_idx = SI_stay_idx & (SI['traj_ind'] == trajid)\n",
    "        UH_traj_idx = UH_stay_idx & (UH['traj_ind'] == trajid)\n",
    "        aPTT_traj_idx = aPTT_stay_idx & (aPTT['traj_ind'] == trajid)\n",
    "        SI_vals = SI[SI_traj_idx].drop(columns=['stay_id','traj_ind']).sort_values(by=['time']).to_numpy()\n",
    "        UH_vals = UH[UH_traj_idx].drop(columns=['stay_id','traj_ind','org_rate']).sort_values(by=['starttime','endtime']).to_numpy()\n",
    "        UH_org_vals = UH[UH_traj_idx].drop(columns=['stay_id','traj_ind','rate']).sort_values(by=['starttime','endtime']).to_numpy()\n",
    "        aPTT_dat = aPTT[aPTT_traj_idx].drop(columns=['stay_id','traj_ind']).sort_values(by=['time']).to_numpy()\n",
    "        aPTT_times = torch.Tensor(aPTT_dat[:,0]).view(-1,1)\n",
    "        aPTT_vals = torch.Tensor(aPTT_dat[:,1]).view(-1,1)\n",
    "        if SI_vals.shape[0] == 0:\n",
    "            # skip\n",
    "            continue\n",
    "        SI_vals = torch.nan_to_num(torch.Tensor(SI_vals),nan=-1.0)\n",
    "        SI_vals[:,1:] = SI_vals[:,1:] + 1.0\n",
    "        data.append({'SI':SI_vals,'times':aPTT_times,'obs':aPTT_vals,'mask':torch.ones_like(aPTT_vals),'U':torch.Tensor(UH_vals),'U_org':torch.Tensor(UH_org_vals)})\n",
    "\n",
    "\n",
    "        \n",
    "# save datasets:\n",
    "test_size = int(test_ratio * len(data))\n",
    "\n",
    "random.shuffle(data)\n",
    "\n",
    "\n",
    "with open(test_out_path,'wb') as f:\n",
    "    pickle.dump({'data':data[:test_size]},f)\n",
    "\n",
    "with open(out_path,'wb') as f:\n",
    "    pickle.dump({'data':data[test_size:]},f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
